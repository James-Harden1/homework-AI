





# 作业i 预训练，训练，推理



## 是什么

#### 微调（训练）是深度学习里的一种重要思想，它是指一个只有少量数据集的任务B，调用（微调或冻结）已经用大量数据训练完成的模型A（任务A的模型）的浅层参数，再**训练**深层参数，调整浅层参数，从而训练出模型B的过程。但A和B任务要相近，这样它们浅层参数才相近。训练出大模型A的过程就是预训练，往往任务A是基础任务，具有跨领域的特征。

#### 预训练用到常见的的算法BERT和GPT。有BERT是运用预训练思想的代表，它用多层enoder把文本理解，特征提取做到了极致，可以非常好地理解语段甚至文章。因此，它有很强的结合性，给它一些性的参数或生成例子，它通过预训练就可以完成各种各样的任务【下游任务改造】。



预训练

1. 训练数据少，不足以训练复杂模型
2. 加快训练速度
3. 参数初始化，找到好的初始点，利于优化

#### 训练的通用流程和总体思路——到底是什么？

模型训练（包含预训练和微调），首先定义一个为完成具体任务（如识别猫狗）的函数$wx+b$【就是一个网络】，用一个数据集里的不同batch输入这个函数，不停生成预测值（与真实值都被转化为数学语言），与真实值计算loss。通过梯度下降（反向传播）决定修改参数w，x的方向，不停迭代，最终减少loss让函数收敛起来。训练模型过程结束后，模型已经找到最优的参数w和x，同时这个网络也达到了能够完成任务的功能。







#### 一个例子通俗直观讲解这三者的主要区别

#### 像学生从小学到高中学习各科基础知识，建立世界观的过程就是预训练；大学生选择专业（如医学、法律），通过专项练习成为专家的过程叫做训练（通常叫做训练后的微调）；像医生用所学知识诊断病人，无需再学习的过程就叫做推理。

#### 对应到深度学习领域

#### 研究者构建BERT的过程就是预训练：培养了一个认识世间万事万物的小孩；微调就是下游任务改造，专注一个领域，再微调数据；推理就是用模型生成结果。

#### 训练相比于预训练：使用更强的正则化，减小学习率，减少数据迭代。【*通过更复杂的模型预训练处理简单任务的模型，效果比直接训练这个模型的数据集好。*】

#### 区别——数据量

#### 一个主要的区别是这三者所需的数据量，实现算力和时间成本。预训练是要训练一个基础的特征提取器，因为要兼顾方方面面，所以它要在巨型计算机上训练数月（因此deepseek公司有通过蒸馏对比快速学习）。而训练只需要少得多的训练量和算力。推理不需要更新参数，大部分算力用于解码输出，在个人电脑上就可以进行。

#### 还有一个区别是预训练是自监督学习，通常是无标签，而训练通常是监督学习，带特定标签或示例输入的过程，训练过程更加优化。











# 作业ii 大模型微调

关键：transforming拓展性

#### 为什么可以微调？：

模型训练分为特征提取和线性分类（softmax）两大块，把特征转化为能在线性分类的语言空间里做线性分割的特征。从浅层到深层——》从基础到高级语义，因此大部分浅层可以共用，直接把语义特征提取层架构复制，从深层（调整较大）往浅层（往往变动较小）调整参数。

#### 

## 微调方式（及其区别）

微调方式有按参数规模分类：FPFT（full parameter fine-tuning ）和PEFT（parameter efficient fine-tuning）前者低参微调，部分更新参数，对参数进行约束，后者全参微调。FPFT计算量较大，但效果也相应更优，PEFT固定部分参数，这样就类似于训练一个小模型，时间较快。

按训练方式分类有SFT（supervised fine-tuning）和fine-tuning，前者是监督学习，后者属于无监督学习。SFT是输入人为标记的数据，例如instruction fine-tuning会输入<指令-输出>示例集，以让模型执行特定任务。





## 微调框架

#### **1. PyTorch**

支持Hugging Face Transformers、PyTorch Lightning等工具链。适合学术研究、中小规模模型微调。需要灵活定制模型结构或训练流程的任务，可操作性强。

- **Transformers**：Hugging Face的预训练模型库，支持BERT、GPT、T5等模型。

  

#### **2. TensorFlow/Keras**

适合于工业级大规模模型训练以及需要与TensorFlow Serving等部署工具深度集成的场景。

#### **3. JAX**

较为专业，极致的调参和科研需求



#### 常用工具

#### DeepSpeed（微软）



- 支持ZeRO（Zero Redundancy Optimizer）优化，大幅降低显存占用。
- 支持3D并行（数据并行、流水线并行、张量并行）。
- 百亿/千亿参数大模型训练（如LLaMA、BLOOM）。

- 资源受限时的高效微调（如单卡微调大模型）。



#### 框架搭配

1. **学术研究/快速原型**：
   - **PyTorch + Hugging Face Transformers + PEFT**（灵活易用，社区支持强）。
2. **工业级大规模训练**：
   - **TensorFlow + DeepSpeed/Megatron-LM**（稳定性高，适合生产部署）。
3. **黑盒模型微调**：
   - **OpenAI API**或**企业级平台**（如百度文心）。





## 总结

大模型不同层对任务的敏感性不同：

- **底层**：捕捉基础特征（如词向量），通常可冻结。
- **中层**：学习语法和上下文关系，部分微调。
- **顶层**：直接关联任务输出（如分类头），需重点调整。



### 大模型微调是将在通用数据上预训练的大模型（如GPT、BERT、LLaMA等）适配到特定任务或领域的关键技术。其核心思想是通过**迁移学习**transforming，利用预训练模型已有的知识，在少量领域数据上调整参数，使模型快速适应新场景。另外，微调对数据质量和对数据覆盖行业度要求较高。



## 大模型微调本质上是尝试减少算力和成本，科学家类比了人脑认识世界的过程（脑神经学），实现训练模型的简化，有时候完成任务的效果比直接训练数据计划好。

### 大模型微调是连接通用智能与 ***垂直场景*** 的桥梁，正是因为它可以应用于各个领域，BERT的出现可以说是里程碑时间。

### 其核心在于**高效利用预训练知识**与**低成本适应新任务**之间的平衡。未来随着PEFT技术的发展与自动化工具的成熟，微调将更加普惠，推动大模型在各行各业落地。















# 作业iii 本地运行大模型在terminal完成交互

我先从Ollama官网下载ollama.exe安装包和大模型model（我选的是千问qwen1.5的一个preview版本，大概1GB多）。

然后在pycharm终端Terminal里运行如下指令：

```输入代码内容
ollama --version
```

检查是否安装完成，成功会输出ollama的版本。

然后下载ollama里的大模型：

```输入命令行
ollama run deepscaler:1.5b-preview-q4_K_M
```

成功后会输出，success，然后直接进入交互界面（ctrl+d退出交互）。

以下是对话示例：

```Terminal交互示例
>>> 给我介绍一下嘻哈音乐
<think>

</think>

**嘻哈音乐**

嘻哈音乐是一种基于节奏和旋律的音乐形式，起源于美国和欧洲。它以流行音乐、舞曲和歌曲为核心，深受年轻人喜爱。以下是关于嘻哈音乐的一些关键点：

### 1. **起源与发展**
   - **美国流行音乐运动**：嘻哈音乐的兴起可以追溯到20世纪80年代的美国流行音乐运动，尤其是1983年的《80th percentile》。
   - **全球影响**：嘻哈音乐在全球范围内广受欢迎，尤其是在亚洲和非洲国家。

### 2. **主要风格**
   - **节奏和节拍**：嘻哈音乐通常以快速节奏开始，比如“5-5”的节奏（每段5秒），导致歌词常常充满夸张或搞笑元素。
   - **旋律和编曲**：许多嘻哈歌曲采用简单的旋律和结构，常结合舞曲元素。

### 3. **代表歌手**
   - **John Legend**：被誉为“流行之神”，他的嘻哈风格广为人知。
   - **Beyoncé**：尽管不是嘻哈音乐的巅峰，她的《Cool Chain》系列歌曲融合了嘻哈元素。
   - **Kendrick Lamar**：是一位以双曲情感和幽默感著称的艺术家。

### 4. **影响与现象**
   - **流行文化**：嘻哈音乐成为流行文化的象征，影响了许多影视作品和音乐组合。
   - **社会现象**：它也被用来探讨性别、种族和社会问题。

### 5. **现代发展**
   - **融合元素**：近年来，嘻哈音乐结合了电子制作、流行舞曲等元素，形成新风格。
   - **全球影响力**：嘻哈音乐正在扩展到欧洲和亚洲地区，成为 these 地区的流行文化主流。

### 总结
嘻哈音乐以其独特的节奏、旋律和歌词形式，成为流行文化的重要组成部分。尽管其巅峰期主要集中在美国，但其影响仍在全球范围内扩散。

>>> During an engine test the torque on the engine is measured to be 33.0 Nm at an  engine speed of 2,875.0rev/min.  Wha
... t is the brake power?
<think>
嗯，好的，我现在要解决的问题是：在一种发动机测试中，测得的 torque 是33.0牛米，在引擎速度为2875 rev/min时，问的是 brake power是多少。

首先，我需要明确一下“ brake power”是什么意思。通常情况下，“power”指的是功率，也就是能量转化的速度。但在机械系统中，尤其是在发动机中，可能会有不同的定义。brake power可能指的是发动机的刹车功率，或者说        
发动机的实际输出功率。

接下来，我要回顾一下相关的物理学公式。在动力学中，功率可以通过 torque 和 angular velocity 来计算。具体来说，功率 P = torque × 角速度。其中，torque 是力矩，angular velocity 是角速度，通常需要用弧度/秒来     
表示。

但是在题目中给出的 engine speed 是 rev/min（转每分钟），也就是 revolutions per minute。我需要将这个转换为弧度/秒，这样才能计算功率。

首先，将 engine speed 转换为 rad/s：

1 revolution = 2π radians，
1 minute = 60 seconds，

所以，2875 rev/min 等于 (2875 × 2π) / 60 rad/s。让我计算一下：

2875 × 2 ≈ 5750，
5750 × π ≈ 5750 × 3.1416 ≈ 18029.5，
然后除以60，得到≈ 300.49 rad/s。

接下来，计算 torque 的功率：P = torque × angular velocity = 33.0 Nm × 300.49 rad/s ≈ 33×300.49 ≈ 9915.27 W（瓦特）。

但通常在发动机中，power可能指的是 brake power，即发动机的实际输出功率。因此，这里的 brake power大约是9915瓦特，也就是约9.9 kW。

不过，我需要确认一下是否正确应用了公式。有时候，在一些工程计算中，可能会将 torque 和 angular velocity 直接相乘来得到功率，而不管单位是否正确。或者，可能还有其他的因素需要考虑，比如效率等。

另外，我还需要检查是否在转换速度时出现了错误：

2875 rev/min = (2875 × 2π) / 60 ≈ (2875 × 6.2832) / 60 ≈ (18249.5)/60 ≈ 304.16 rad/s。这可能更接近正确值。

所以，重新计算：

P = 33 Nm × 304.16 rad/s ≈ 33×304.16 ≈ 10,037 W 或者约10.0 kW。

不过，通常在实际应用中，功率可能更精确地表示为9925 W（约9.92 kW）。所以，最终的答案可能是大约9.9 kW的 brake power。
</think>

为了计算发动机的刹车功率，我们首先将给定的 engine speed 转换为弧度/秒：

1 revolution = 2π radians，
因此：
\[ 2875 \, \text{rev/min} = \frac{2875 \times 2\pi}{60} \, \text{rad/s} \approx 304.16 \, \text{rad/s} \]

接下来，计算功率（P）：
\[ P = \text{torque} \times \text{angular velocity} = 33.0 \, \text{Nm} \times 304.16 \, \text{rad/s} \approx 9925 \, \text{W} \]
即约：
\[ P \approx 9.925 \, \text{kW} \]

因此，发动机的 brake power约为：

\boxed{9.93 \, \text{kW}}

```

事实证明，qwen1.5支持中文及其英文的读取和输出，而且模型相当于导到本地了，因此，不联网也可以正常输出结果。而同样的两个问题，我在cmd命令行和pycharm分别运行，pycharm第一题的输出更具逻辑性，第二道热力学题目也做对了，而cmd却出错了。（不知道这是什么原理，两个结果不一样，差异尽较大）

![image-20250406230047938](C:\Users\20786\AppData\Roaming\Typora\typora-user-images\image-20250406230047938.png)

![](C:\Users\20786\Pictures\Screenshots\屏幕截图 2025-04-06 230013.png)



